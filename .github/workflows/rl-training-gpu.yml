name: RL Training (GPU)

on:
  push:
    paths:
      - 'blokus-engine/training_queue.json'
    branches:
      - 'xp/**'
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write
  actions: write

env:
  PYTHON_VERSION: "3.11"

jobs:
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Parse training queue
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  setup:
    name: ğŸ”§ Parse training configuration
    runs-on: ubuntu-latest
    outputs:
      experiment_name: ${{ steps.parse.outputs.experiment_name }}
      board_size: ${{ steps.parse.outputs.board_size }}
      episodes: ${{ steps.parse.outputs.episodes }}
      eval_frequency: ${{ steps.parse.outputs.eval_frequency }}
      eval_games: ${{ steps.parse.outputs.eval_games }}
      log_frequency: ${{ steps.parse.outputs.log_frequency }}
      min_buffer_size: ${{ steps.parse.outputs.min_buffer_size }}
      resume: ${{ steps.parse.outputs.resume }}
      device: ${{ steps.parse.outputs.device }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Parse training queue
        id: parse
        run: |
          QUEUE_FILE="blokus-engine/training_queue.json"
          
          if [ ! -f "$QUEUE_FILE" ]; then
            echo "âŒ Training queue file not found"
            exit 1
          fi
          
          QUEUE_LENGTH=$(jq '.queue | length' "$QUEUE_FILE")
          
          if [ "$QUEUE_LENGTH" -eq 0 ]; then
            echo "â„¹ï¸ Training queue is empty, skipping"
            echo "experiment_name=none" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          
          # Get first item from queue
          EXP_NAME=$(jq -r '.queue[0].experiment_name' "$QUEUE_FILE")
          BOARD_SIZE=$(jq -r '.queue[0].board_size // "14"' "$QUEUE_FILE")
          EPISODES=$(jq -r '.queue[0].episodes // "1000"' "$QUEUE_FILE")
          EVAL_FREQ=$(jq -r '.queue[0].eval_frequency // "100"' "$QUEUE_FILE")
          EVAL_GAMES=$(jq -r '.queue[0].eval_games // "50"' "$QUEUE_FILE")
          LOG_FREQ=$(jq -r '.queue[0].log_frequency // "10"' "$QUEUE_FILE")
          MIN_BUFFER=$(jq -r '.queue[0].min_buffer_size // "1000"' "$QUEUE_FILE")
          RESUME=$(jq -r '.queue[0].resume // "false"' "$QUEUE_FILE")
          DEVICE=$(jq -r '.queue[0].device // "cpu"' "$QUEUE_FILE")
          
          echo "ğŸ“‹ Training Configuration:"
          echo "  Experiment: $EXP_NAME"
          echo "  Board: ${BOARD_SIZE}x${BOARD_SIZE}"
          echo "  Episodes: $EPISODES"
          echo "  Eval Frequency: $EVAL_FREQ"
          echo "  Eval Games: $EVAL_GAMES"
          echo "  Log Frequency: $LOG_FREQ"
          echo "  Min Buffer: $MIN_BUFFER"
          echo "  Resume: $RESUME"
          echo "  Device: $DEVICE"
          
          echo "experiment_name=$EXP_NAME" >> "$GITHUB_OUTPUT"
          echo "board_size=$BOARD_SIZE" >> "$GITHUB_OUTPUT"
          echo "episodes=$EPISODES" >> "$GITHUB_OUTPUT"
          echo "eval_frequency=$EVAL_FREQ" >> "$GITHUB_OUTPUT"
          echo "eval_games=$EVAL_GAMES" >> "$GITHUB_OUTPUT"
          echo "log_frequency=$LOG_FREQ" >> "$GITHUB_OUTPUT"
          echo "min_buffer_size=$MIN_BUFFER" >> "$GITHUB_OUTPUT"
          echo "resume=$RESUME" >> "$GITHUB_OUTPUT"
          echo "device=$DEVICE" >> "$GITHUB_OUTPUT"

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # GPU Training on Occidata
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  train:
    name: ğŸš€ RL Training (GPU)
    runs-on: [self-hosted, blokus, occidata]
    needs: [setup]
    if: needs.setup.outputs.experiment_name != 'none' && needs.setup.outputs.device == 'gpu'
    
    env:
      JULIA_DEPOT_PATH: /projects/ctb/blokus-runner/julia_depot
      
    outputs:
      experiment_dir: ${{ steps.collect.outputs.experiment_dir }}
      best_win_rate: ${{ steps.collect.outputs.best_win_rate }}
      success: ${{ steps.collect.outputs.success }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4


      - name: ğŸ“¦ Setup Python environment
        run: |
          # Use persistent venv (pre-installed on runner)
          VENV_DIR="/projects/ctb/blokus-runner/blokus-venv"
          
          if [ ! -d "$VENV_DIR" ]; then
            echo "âŒ Persistent venv not found at $VENV_DIR"
            echo "   Please run: bash scripts/setup_runner_env.sh on the runner"
            exit 1
          fi
          
          source "$VENV_DIR/bin/activate"
          echo "âœ… Using persistent venv: $VENV_DIR"
          echo "   Python: $(python --version)"
        
          # Upgrade pip to ensure support for modern pyproject.toml editable installs
          pip install --upgrade pip
        
          # Install/update blokus package (fast since deps are already installed)
          cd blokus-engine
          pip install -e ".[dev]" --quiet
          
          # Verify PyTorch
          python -c "import torch; print(f'âœ… PyTorch {torch.__version__} ready')"

      - name: ğŸ‹ï¸ Dispatch training to SLURM
        run: |
          cd blokus-engine
          WORKDIR=$(pwd)
          
          # Create SLURM dispatch script
          cat > dispatch_training.sh <<'EOF'
          #!/bin/bash
          set -euo pipefail
          
          WORKDIR="$1"
          SCRIPT_PATH="$2"
          EXP_NAME="$3"
          BOARD_SIZE="$4"
          EPISODES="$5"
          EVAL_FREQ="$6"
          EVAL_GAMES="$7"
          LOG_FREQ="$8"
          MIN_BUFFER="$9"
          
          JOB_SCRIPT=$(mktemp)
          cat > "$JOB_SCRIPT" <<SLURM
          #!/bin/bash
          #SBATCH --job-name=blokus-rl
          #SBATCH --partition=GPUNodes
          #SBATCH --gres=gpu:1080ti:1
          #SBATCH --gres-flags=enforce-binding
          #SBATCH --cpus-per-task=4
          #SBATCH --mem=32G
          #SBATCH --time=06:00:00
          #SBATCH --output=$WORKDIR/slurm-%j.out
          #SBATCH --error=$WORKDIR/slurm-%j.err
          
          set -exo pipefail
          
          echo "=========================================="
          echo "Blokus RL Training on GPU"
          echo "=========================================="
          echo "Job ID: \$SLURM_JOB_ID"
          echo "Node: \$SLURM_NODELIST"
          echo "Working directory: $WORKDIR"
          echo ""
          
          # Singularity container path
          CONTAINER="/apps/containerCollections/CUDA12/pytorch2-NGC-24-02.sif"
          VENV_DIR="/projects/ctb/blokus-runner/blokus-venv"
          
          echo "Step 1: Changing to working directory..."
          cd $WORKDIR
          echo "Current directory: \$(pwd)"
          echo ""
          
          echo "Step 2: GPU Information (via Singularity)..."
          singularity exec \$CONTAINER nvidia-smi
          echo ""
          
          echo "Step 3: Listing files..."
          ls -la
          echo ""
          
          echo "Step 4: Starting training in Singularity container..."
          echo "Container: \$CONTAINER"
          echo "Venv: \$VENV_DIR"
          echo "Script: $SCRIPT_PATH"
          echo "Experiment: $EXP_NAME"
          echo ""
          
          # Run training inside Singularity container with venv activated
          singularity exec \$CONTAINER bash -c "
            set -exo pipefail
            source \$VENV_DIR/bin/activate
            echo 'Python: '\$(which python)
            echo 'Python version: '\$(python --version)
            echo 'PyTorch version: '\$(python -c 'import torch; print(torch.__version__)')
            echo 'CUDA available: '\$(python -c 'import torch; print(torch.cuda.is_available())')
            echo ''
            echo 'Starting training...'
            python -u $SCRIPT_PATH --new --name $EXP_NAME --board-size $BOARD_SIZE --episodes $EPISODES --eval-freq $EVAL_FREQ --eval-games $EVAL_GAMES --log-freq $LOG_FREQ --min-buffer $MIN_BUFFER --no-video | tee training_output.txt
          "
          
          echo ""
          echo "Step 5: Processing results..."
          ACTUAL_EXP_DIR=\$(grep "EXPERIMENT_DIR=" training_output.txt | cut -d'=' -f2)
          echo "Experiment directory: \$ACTUAL_EXP_DIR"
          echo "EXPERIMENT_DIR=\$ACTUAL_EXP_DIR" > job_results.txt
          
          METADATA_FILE="\$ACTUAL_EXP_DIR/metadata.json"
          if [ -f "\$METADATA_FILE" ]; then
            BEST_WIN_RATE=\$(python -c "import json; print(json.load(open('\$METADATA_FILE'))['best_win_rate'])")
            echo "Best win rate: \$BEST_WIN_RATE"
            echo "BEST_WIN_RATE=\$BEST_WIN_RATE" >> job_results.txt
          else
            echo "âš ï¸  Metadata file not found"
            echo "BEST_WIN_RATE=0.0" >> job_results.txt
          fi
          
          echo ""
          echo "=========================================="
          echo "âœ… SLURM Job Completed Successfully"
          echo "=========================================="
          SLURM
          
          JOB_ID=$(sbatch --parsable "$JOB_SCRIPT")
          echo "ğŸ†” SLURM Job ID: $JOB_ID"
          echo "ğŸ“‚ Output file: $WORKDIR/slurm-${JOB_ID}.out"
          echo "ğŸ“‚ Error file: $WORKDIR/slurm-${JOB_ID}.err"
          
          # Wait for output file to be created
          echo "â³ Waiting for SLURM job to start..."
          WAIT_COUNT=0
          while [ ! -f "$WORKDIR/slurm-${JOB_ID}.out" ] && [ $WAIT_COUNT -lt 60 ]; do
            sleep 2
            WAIT_COUNT=$((WAIT_COUNT + 1))
          done
          
          if [ -f "$WORKDIR/slurm-${JOB_ID}.out" ]; then
            echo "âœ… Output file created, streaming output..."
            echo "=========================================="
            
            # Tail output in background
            tail -f "$WORKDIR/slurm-${JOB_ID}.out" &
            TAIL_PID=$!
            
            # Wait for job completion
            while squeue -j $JOB_ID 2>/dev/null | grep -q $JOB_ID; do
              sleep 30
            done
            
            # Kill tail process
            kill $TAIL_PID 2>/dev/null || true
            sleep 1
            
            echo "=========================================="
            echo "ğŸ“‹ Final output (ensuring nothing was missed):"
            cat "$WORKDIR/slurm-${JOB_ID}.out"
          else
            echo "âš ï¸  Output file not created after 2 minutes"
            echo "Checking job status..."
          fi
          
          # Check status
          echo ""
          
          # Always show stderr for debugging
          if [ -s "$WORKDIR/slurm-${JOB_ID}.err" ]; then
            echo "âš ï¸  SLURM Job Errors/Warnings:"
            cat "$WORKDIR/slurm-${JOB_ID}.err"
            echo ""
          fi
          
          if sacct -j $JOB_ID --format=State --noheader | grep -q "COMPLETED"; then
            echo "âœ… Job completed successfully"
            
            # Debug: show what files were created
            echo ""
            echo "ğŸ“ Files in workspace:"
            ls -lh "$WORKDIR/" | grep -E "(slurm|job_results|training_output)" || echo "No relevant files found"
            
            # Check if output is actually empty
            if [ ! -s "$WORKDIR/slurm-${JOB_ID}.out" ]; then
              echo ""
              echo "âŒ WARNING: Output file is empty!"
              echo "This usually means the SLURM job failed to execute properly."
              exit 1
            fi
            
            exit 0
          else
            echo "âŒ Job failed"
            echo "ğŸ“‹ SLURM Job Errors:"
            cat "$WORKDIR/slurm-${JOB_ID}.err" 2>/dev/null || echo "(no stderr)"
            exit 1
          fi
          EOF
          
          chmod +x dispatch_training.sh
          ./dispatch_training.sh \
            "$WORKDIR" \
            "scripts/train.py" \
            "${{ needs.setup.outputs.experiment_name }}" \
            "${{ needs.setup.outputs.board_size }}" \
            "${{ needs.setup.outputs.episodes }}" \
            "${{ needs.setup.outputs.eval_frequency }}" \
            "${{ needs.setup.outputs.eval_games }}" \
            "${{ needs.setup.outputs.log_frequency }}" \
            "${{ needs.setup.outputs.min_buffer_size }}"

      - name: ğŸ“Š Collect results
        id: collect
        if: always()
        run: |
          cd blokus-engine
          
          if [ -f "job_results.txt" ]; then
            source job_results.txt
            echo "experiment_dir=$EXPERIMENT_DIR" >> "$GITHUB_OUTPUT"
            echo "best_win_rate=$BEST_WIN_RATE" >> "$GITHUB_OUTPUT"
            echo "success=true" >> "$GITHUB_OUTPUT"
            
            # Generate plots
            source /projects/ctb/blokus-runner/blokus-venv/bin/activate
            METRICS_FILE="$EXPERIMENT_DIR/metrics.csv"
            if [ -f "$METRICS_FILE" ]; then
              python scripts/plot_metrics.py "$METRICS_FILE" "$EXPERIMENT_DIR/training_plot.png"
            fi
          else
            echo "success=false" >> "$GITHUB_OUTPUT"
          fi

      - name: ğŸ“¦ Upload training artifacts
        if: steps.collect.outputs.success == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: training-results-gpu
          path: |
            ${{ steps.collect.outputs.experiment_dir }}/model.pt
            ${{ steps.collect.outputs.experiment_dir }}/metadata.json
            ${{ steps.collect.outputs.experiment_dir }}/metrics.csv
            ${{ steps.collect.outputs.experiment_dir }}/training_plot.png
          retention-days: 30

      - name: ğŸš€ Deploy model to registry
        id: deploy
        if: steps.collect.outputs.success == 'true'
        run: |
          cd blokus-engine
          source job_results.txt
          
          # Quality gate: only deploy if win_rate >= 0.5
          WIN_RATE_THRESHOLD=0.5
          if (( $(echo "$BEST_WIN_RATE >= $WIN_RATE_THRESHOLD" | bc -l) )); then
            echo "âœ… Model meets quality threshold (win_rate: $BEST_WIN_RATE >= $WIN_RATE_THRESHOLD)"
            
            # Create deployed directory if it doesn't exist
            mkdir -p models/deployed
            
            # Copy model to deployed directory
            EXP_NAME="${{ needs.setup.outputs.experiment_name }}"
            DEPLOYED_PATH="models/deployed/${EXP_NAME}.pt"
            cp "$EXPERIMENT_DIR/model.pt" "$DEPLOYED_PATH"
            echo "ğŸ“¦ Copied model to $DEPLOYED_PATH"
            
            # Update registry
            python ../scripts/update_registry.py \
              --id "$EXP_NAME" \
              --name "Expert Duo ($EXP_NAME)" \
              --description "IA entraÃ®nÃ©e sur GPU (Occidata). Win rate: ${BEST_WIN_RATE}. Niveau expert." \
              --model-path "deployed/${EXP_NAME}.pt" \
              --board-size ${{ needs.setup.outputs.board_size }} \
              --enabled
            
            echo "deployed=true" >> "$GITHUB_OUTPUT"
            echo "deployed_path=$DEPLOYED_PATH" >> "$GITHUB_OUTPUT"
          else
            echo "âš ï¸  Model does not meet quality threshold (win_rate: $BEST_WIN_RATE < $WIN_RATE_THRESHOLD)"
            echo "Model will not be deployed to registry"
            echo "deployed=false" >> "$GITHUB_OUTPUT"
          fi


  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Commit results
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  commit-results:
    name: ğŸ’¾ Commit training results
    runs-on: ubuntu-latest
    needs: [setup, train]
    if: needs.train.outputs.success == 'true'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download training artifacts
        uses: actions/download-artifact@v4
        with:
          name: training-results-gpu
          path: ${{ needs.train.outputs.experiment_dir }}

      - name: Commit and push results
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Add experiment results
          git add ${{ needs.train.outputs.experiment_dir }}
          
          # Add deployed model and registry if deployment happened
          if [ -f "blokus-engine/models/deployed/${{ needs.setup.outputs.experiment_name }}.pt" ]; then
            git add blokus-engine/models/deployed/${{ needs.setup.outputs.experiment_name }}.pt
            git add blokus-engine/models/registry.json
            
            git commit -m "ğŸš€ Deploy model: ${{ needs.setup.outputs.experiment_name }}
          
          Win rate: ${{ needs.train.outputs.best_win_rate }}
          Trained on: Occidata GPU (GTX 1080 Ti)
          
          Model deployed to registry and ready to play in browser!" || echo "No changes to commit"
          else
            git commit -m "ğŸ¤– GPU Training results: ${{ needs.train.outputs.experiment_dir }}
          
          Best win rate: ${{ needs.train.outputs.best_win_rate }}
          Trained on: Occidata GPU (GTX 1080 Ti)
          
          Model did not meet deployment threshold." || echo "No changes to commit"
          fi
          
          git push || echo "Nothing to push"

