name: RL Training (GPU)

on:
  push:
    paths:
      - 'blokus-engine/training_queue.json'
    branches:
      - 'xp/**'
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write
  actions: write

env:
  PYTHON_VERSION: "3.11"

jobs:
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Parse training queue
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  setup:
    name: ğŸ”§ Parse training configuration
    runs-on: ubuntu-latest
    outputs:
      experiment_name: ${{ steps.parse.outputs.experiment_name }}
      board_size: ${{ steps.parse.outputs.board_size }}
      episodes: ${{ steps.parse.outputs.episodes }}
      eval_frequency: ${{ steps.parse.outputs.eval_frequency }}
      eval_games: ${{ steps.parse.outputs.eval_games }}
      log_frequency: ${{ steps.parse.outputs.log_frequency }}
      min_buffer_size: ${{ steps.parse.outputs.min_buffer_size }}
      resume: ${{ steps.parse.outputs.resume }}
      device: ${{ steps.parse.outputs.device }}
      epsilon_decay: ${{ steps.parse.outputs.epsilon_decay }}
      learning_rate: ${{ steps.parse.outputs.learning_rate }}
      batch_size: ${{ steps.parse.outputs.batch_size }}
      upload_model: ${{ steps.parse.outputs.upload_model }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Parse training queue
        id: parse
        run: |
          QUEUE_FILE="blokus-engine/training_queue.json"
          
          if [ ! -f "$QUEUE_FILE" ]; then
            echo "âŒ Training queue file not found"
            exit 1
          fi
          
          QUEUE_LENGTH=$(jq '.queue | length' "$QUEUE_FILE")
          
          if [ "$QUEUE_LENGTH" -eq 0 ]; then
            echo "â„¹ï¸ Training queue is empty, skipping"
            echo "experiment_name=none" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          
          # Get first item from queue
          EXP_NAME=$(jq -r '.queue[0].experiment_name' "$QUEUE_FILE")
          BOARD_SIZE=$(jq -r '.queue[0].board_size // "14"' "$QUEUE_FILE")
          EPISODES=$(jq -r '.queue[0].episodes // "1000"' "$QUEUE_FILE")
          EVAL_FREQ=$(jq -r '.queue[0].eval_frequency // "100"' "$QUEUE_FILE")
          EVAL_GAMES=$(jq -r '.queue[0].eval_games // "50"' "$QUEUE_FILE")
          LOG_FREQ=$(jq -r '.queue[0].log_frequency // "10"' "$QUEUE_FILE")
          MIN_BUFFER=$(jq -r '.queue[0].min_buffer_size // "1000"' "$QUEUE_FILE")
          RESUME=$(jq -r '.queue[0].resume // "false"' "$QUEUE_FILE")
          DEVICE=$(jq -r '.queue[0].device // "cpu"' "$QUEUE_FILE")
          EPSILON_DECAY=$(jq -r '.queue[0].epsilon_decay // "50000"' "$QUEUE_FILE")
          LEARNING_RATE=$(jq -r '.queue[0].learning_rate // "0.0001"' "$QUEUE_FILE")
          BATCH_SIZE=$(jq -r '.queue[0].batch_size // "64"' "$QUEUE_FILE")
          UPLOAD_MODEL=$(jq -r 'if .queue[0].upload_model == null then "true" else .queue[0].upload_model end' "$QUEUE_FILE")
          
          echo "ğŸ“‹ Training Configuration:"
          echo "  Experiment: $EXP_NAME"
          echo "  Board: ${BOARD_SIZE}x${BOARD_SIZE}"
          echo "  Episodes: $EPISODES"
          echo "  Eval Frequency: $EVAL_FREQ"
          echo "  Eval Games: $EVAL_GAMES"
          echo "  Log Frequency: $LOG_FREQ"
          echo "  Min Buffer: $MIN_BUFFER"
          echo "  Resume: $RESUME"
          echo "  Device: $DEVICE"
          echo "  Epsilon Decay: $EPSILON_DECAY"
          echo "  Learning Rate: $LEARNING_RATE"
          echo "  Batch Size: $BATCH_SIZE"
          echo "  Upload Model: $UPLOAD_MODEL"
          
          echo "experiment_name=$EXP_NAME" >> "$GITHUB_OUTPUT"
          echo "board_size=$BOARD_SIZE" >> "$GITHUB_OUTPUT"
          echo "episodes=$EPISODES" >> "$GITHUB_OUTPUT"
          echo "eval_frequency=$EVAL_FREQ" >> "$GITHUB_OUTPUT"
          echo "eval_games=$EVAL_GAMES" >> "$GITHUB_OUTPUT"
          echo "log_frequency=$LOG_FREQ" >> "$GITHUB_OUTPUT"
          echo "min_buffer_size=$MIN_BUFFER" >> "$GITHUB_OUTPUT"
          echo "resume=$RESUME" >> "$GITHUB_OUTPUT"
          echo "device=$DEVICE" >> "$GITHUB_OUTPUT"
          echo "epsilon_decay=$EPSILON_DECAY" >> "$GITHUB_OUTPUT"
          echo "learning_rate=$LEARNING_RATE" >> "$GITHUB_OUTPUT"
          echo "batch_size=$BATCH_SIZE" >> "$GITHUB_OUTPUT"
          echo "upload_model=$UPLOAD_MODEL" >> "$GITHUB_OUTPUT"

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # GPU Training on Occidata
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  train:
    name: ğŸš€ RL Training (GPU)
    runs-on: [self-hosted, blokus, occidata]
    needs: [setup]
    if: needs.setup.outputs.experiment_name != 'none' && needs.setup.outputs.device == 'gpu'
    timeout-minutes: 1440
    
    env:
      JULIA_DEPOT_PATH: /projects/ctb/blokus-runner/julia_depot
      
    outputs:
      experiment_dir: ${{ steps.collect.outputs.experiment_dir }}
      best_win_rate: ${{ steps.collect.outputs.best_win_rate }}
      success: ${{ steps.collect.outputs.success }}
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4


      - name: ğŸ“¦ Setup Python environment
        run: |
          # Use persistent venv (pre-installed on runner)
          VENV_DIR="/projects/ctb/blokus-runner/blokus-venv"
          
          if [ ! -d "$VENV_DIR" ]; then
            echo "âŒ Persistent venv not found at $VENV_DIR"
            echo "   Please run: bash scripts/setup_runner_env.sh on the runner"
            exit 1
          fi
          
          source "$VENV_DIR/bin/activate"
          echo "âœ… Using persistent venv: $VENV_DIR"
          echo "   Python: $(python --version)"
        
          # Upgrade pip to ensure support for modern pyproject.toml editable installs
          pip install --upgrade pip
        
          # Install/update blokus package (fast since deps are already installed)
          cd blokus-engine
          pip install -e ".[dev]" --quiet
          
          # Verify PyTorch
          python -c "import torch; print(f'âœ… PyTorch {torch.__version__} ready')"

      - name: ğŸ‹ï¸ Dispatch training to SLURM
        id: dispatch
        run: |
          cd blokus-engine
          WORKDIR=$(pwd)
          
          # Handle Resume Logic
          EXP_NAME="${{ needs.setup.outputs.experiment_name }}"
          RESUME="${{ needs.setup.outputs.resume }}"
          
          if [ "$RESUME" = "true" ]; then
            echo "ğŸ”„ Resume requested. Searching for previous checkpoint..."
            PERSISTENT_BASE="/projects/ctb/blokus-runner/trained_models"
            
            # Find latest experiment folder matching the name
            # Format is usually experiment_name_YYYYMMDD_HHMMSS
            LATEST_EXP=$(find "$PERSISTENT_BASE" -maxdepth 1 -name "${EXP_NAME}_*" -type d | sort -r | head -n 1)
            
            if [ -n "$LATEST_EXP" ]; then
              echo "âœ… Found previous experiment: $LATEST_EXP"
              EXP_FOLDER=$(basename "$LATEST_EXP")
              
              # Create experiments directory if needed
              mkdir -p "models/experiments"
              
              # Copy from persistent storage to workspace
              cp -r "$LATEST_EXP" "models/experiments/"
              
              echo "ğŸ“‚ Restored checkpoint to models/experiments/$EXP_FOLDER"
              ls -lh "models/experiments/$EXP_FOLDER/"
              
              # UPDATE EXP_NAME to use the specific folder name so train.py finds it
              EXP_NAME="$EXP_FOLDER"
              echo "ğŸ¯ targeted experiment set to: $EXP_NAME"
            else
              echo "âŒ No previous experiment found for $EXP_NAME in $PERSISTENT_BASE"
              echo "Cannot resume. Aborting."
              exit 1
            fi
          fi
          
          # Create SLURM dispatch script
          cat > dispatch_training.sh <<'EOF'
          #!/bin/bash
          set -euo pipefail
          
          WORKDIR="$1"
          SCRIPT_PATH="$2"
          EXP_NAME="$3"
          BOARD_SIZE="$4"
          EPISODES="$5"
          EVAL_FREQ="$6"
          EVAL_GAMES="$7"
          LOG_FREQ="$8"
          MIN_BUFFER="$9"
          EPSILON_DECAY="${10}"
          LEARNING_RATE="${11}"
          BATCH_SIZE="${12}"
          
          JOB_SCRIPT=$(mktemp)
          cat > "$JOB_SCRIPT" <<SLURM
          #!/bin/bash
          #SBATCH --job-name=blokus-rl
          #SBATCH --partition=GPUNodes
          #SBATCH --gres=gpu:1080ti:1
          #SBATCH --gres-flags=enforce-binding
          #SBATCH --cpus-per-task=4
          #SBATCH --mem=32G
          #SBATCH --time=06:00:00
          #SBATCH --output=$WORKDIR/slurm-%j.out
          #SBATCH --error=$WORKDIR/slurm-%j.err
          
          set -exo pipefail
          
          echo "=========================================="
          echo "Blokus RL Training on GPU"
          echo "=========================================="
          echo "Job ID: \$SLURM_JOB_ID"
          echo "Node: \$SLURM_NODELIST"
          echo "Working directory: $WORKDIR"
          echo ""
          
          # Singularity container path
          CONTAINER="/apps/containerCollections/CUDA12/pytorch2-NGC-24-02.sif"
          VENV_DIR="/projects/ctb/blokus-runner/blokus-venv"
          
          echo "Step 1: Changing to working directory..."
          cd $WORKDIR
          echo "Current directory: \$(pwd)"
          echo ""
          
          echo "Step 2: GPU Information (via Singularity)..."
          singularity exec \$CONTAINER nvidia-smi
          echo ""
          
          echo "Step 3: Listing files..."
          ls -la
          echo ""
          
          echo "Step 4: Starting training in Singularity container..."
          echo "Container: \$CONTAINER"
          echo "Venv: \$VENV_DIR"
          echo "Script: $SCRIPT_PATH"
          echo "Experiment: $EXP_NAME"
          echo ""
          
          # Run training inside Singularity container with venv activated
          singularity exec \$CONTAINER bash -c "
            set -exo pipefail
            source \$VENV_DIR/bin/activate
            echo 'Python: '\$(which python)
            echo 'Python version: '\$(python --version)
            echo 'PyTorch version: '\$(python -c 'import torch; print(torch.__version__)')
            echo 'CUDA available: '\$(python -c 'import torch; print(torch.cuda.is_available())')
            echo ''
            echo 'Starting training...'
            python -u $SCRIPT_PATH --new --name $EXP_NAME --board-size $BOARD_SIZE --episodes $EPISODES --eval-freq $EVAL_FREQ --eval-games $EVAL_GAMES --log-freq $LOG_FREQ --min-buffer $MIN_BUFFER --epsilon-decay $EPSILON_DECAY --lr $LEARNING_RATE --batch-size $BATCH_SIZE --no-video | tee training_output.txt
          "
          
          echo ""
          echo "Step 5: Processing results..."
          ACTUAL_EXP_DIR=\$(grep "EXPERIMENT_DIR=" training_output.txt | cut -d'=' -f2)
          echo "Experiment directory: \$ACTUAL_EXP_DIR"
          echo "EXPERIMENT_DIR=\$ACTUAL_EXP_DIR" > job_results.txt
          
          METADATA_FILE="\$ACTUAL_EXP_DIR/metadata.json"
          if [ -f "\$METADATA_FILE" ]; then
            BEST_WIN_RATE=\$(python -c "import json; print(json.load(open('\$METADATA_FILE'))['best_win_rate'])")
            echo "Best win rate: \$BEST_WIN_RATE"
            echo "BEST_WIN_RATE=\$BEST_WIN_RATE" >> job_results.txt
          else
            echo "âš ï¸  Metadata file not found"
            echo "BEST_WIN_RATE=0.0" >> job_results.txt
          fi
          
          echo ""
          echo "=========================================="
          echo "âœ… SLURM Job Completed Successfully"
          echo "=========================================="
          SLURM
          
          JOB_ID=$(sbatch --parsable "$JOB_SCRIPT")
          echo "ğŸ†” SLURM Job ID: $JOB_ID"
          echo "ğŸ“‚ Output file: $WORKDIR/slurm-${JOB_ID}.out"
          echo "ğŸ“‚ Error file: $WORKDIR/slurm-${JOB_ID}.err"
          
          # Wait for output file to be created
          echo "â³ Waiting for SLURM job to start..."
          WAIT_COUNT=0
          while [ ! -f "$WORKDIR/slurm-${JOB_ID}.out" ] && [ $WAIT_COUNT -lt 60 ]; do
            sleep 2
            WAIT_COUNT=$((WAIT_COUNT + 1))
          done
          
          if [ -f "$WORKDIR/slurm-${JOB_ID}.out" ]; then
            echo "âœ… Output file created, streaming output..."
            echo "=========================================="
            
            # Tail output in background
            tail -f "$WORKDIR/slurm-${JOB_ID}.out" &
            TAIL_PID=$!
            
            # Wait for job completion
            while squeue -j $JOB_ID 2>/dev/null | grep -q $JOB_ID; do
              sleep 30
            done
            
            # Kill tail process
            kill $TAIL_PID 2>/dev/null || true
            sleep 1
            
            echo "=========================================="
            echo "ğŸ“‹ Final output (ensuring nothing was missed):"
            cat "$WORKDIR/slurm-${JOB_ID}.out"
          else
            echo "âš ï¸  Output file not created after 2 minutes"
            echo "Checking job status..."
          fi
          
          # Check status
          echo ""
          
          # Always show stderr for debugging
          if [ -s "$WORKDIR/slurm-${JOB_ID}.err" ]; then
            echo "âš ï¸  SLURM Job Errors/Warnings:"
            cat "$WORKDIR/slurm-${JOB_ID}.err"
            echo ""
          fi
          
          if sacct -j $JOB_ID --format=State --noheader | grep -q "COMPLETED"; then
            echo "âœ… Job completed successfully"
            
            # Debug: show what files were created
            echo ""
            echo "ğŸ“ Files in workspace:"
            ls -lh "$WORKDIR/" | grep -E "(slurm|job_results|training_output)" || echo "No relevant files found"
            
            # Check if output is actually empty
            if [ ! -s "$WORKDIR/slurm-${JOB_ID}.out" ]; then
              echo ""
              echo "âŒ WARNING: Output file is empty!"
              echo "This usually means the SLURM job failed to execute properly."
              exit 1
            fi
            
            exit 0
          else
            echo "âŒ Job failed"
            echo "ğŸ“‹ SLURM Job Errors:"
            cat "$WORKDIR/slurm-${JOB_ID}.err" 2>/dev/null || echo "(no stderr)"
            exit 1
          fi
          EOF
          
          chmod +x dispatch_training.sh
          ./dispatch_training.sh \
            "$WORKDIR" \
            "scripts/train.py" \
            "$EXP_NAME" \
            "${{ needs.setup.outputs.board_size }}" \
            "${{ needs.setup.outputs.episodes }}" \
            "${{ needs.setup.outputs.eval_frequency }}" \
            "${{ needs.setup.outputs.eval_games }}" \
            "${{ needs.setup.outputs.log_frequency }}" \
            "${{ needs.setup.outputs.min_buffer_size }}" \
            "${{ needs.setup.outputs.epsilon_decay }}" \
            "${{ needs.setup.outputs.learning_rate }}" \
            "${{ needs.setup.outputs.batch_size }}"

      - name: ğŸ“Š Collect results
        id: collect
        if: always()
        run: |
          cd blokus-engine
          
          if [ -f "job_results.txt" ]; then
            source job_results.txt
            echo "experiment_dir=$EXPERIMENT_DIR" >> "$GITHUB_OUTPUT"
            echo "best_win_rate=$BEST_WIN_RATE" >> "$GITHUB_OUTPUT"
            echo "success=true" >> "$GITHUB_OUTPUT"
            
            # Generate plots
            source /projects/ctb/blokus-runner/blokus-venv/bin/activate
            METRICS_FILE="$EXPERIMENT_DIR/metrics.csv"
            if [ -f "$METRICS_FILE" ]; then
              python scripts/plot_metrics.py "$METRICS_FILE" "$EXPERIMENT_DIR/training_plot.png"
            fi
            
            # Prepare model artifact
            if [ -f "$EXPERIMENT_DIR/checkpoint_best.pt" ]; then
              cp "$EXPERIMENT_DIR/checkpoint_best.pt" "$EXPERIMENT_DIR/model.pt"
              echo "âœ… Copied best checkpoint to model.pt"
            elif [ -f "$EXPERIMENT_DIR/checkpoint_latest.pt" ]; then
              cp "$EXPERIMENT_DIR/checkpoint_latest.pt" "$EXPERIMENT_DIR/model.pt"
              echo "âš ï¸  Copied latest checkpoint to model.pt (best not found)"
            else
              echo "âŒ No checkpoint found to copy!"
            fi
          else
            echo "success=false" >> "$GITHUB_OUTPUT"
          fi

      - name: ğŸ’¾ Save model to persistent storage
        if: steps.collect.outputs.success == 'true'
        run: |
          EXP_DIR="${{ steps.collect.outputs.experiment_dir }}"
          EXP_NAME=$(basename "$EXP_DIR")
          PERSISTENT_DIR="/projects/ctb/blokus-runner/trained_models/$EXP_NAME"
          
          echo "ğŸ“¦ Copying results to persistent storage..."
          mkdir -p "$PERSISTENT_DIR"
          cp -r "$EXP_DIR"/* "$PERSISTENT_DIR/"
          
          echo "âœ… Model saved to: $PERSISTENT_DIR"
          echo "persistent_dir=$PERSISTENT_DIR" >> "$GITHUB_OUTPUT"
          ls -lh "$PERSISTENT_DIR/"

      - name: ğŸ“¦ Prepare artifacts for upload (lightweight, no model.pt)
        if: steps.collect.outputs.success == 'true'
        run: |
          mkdir -p ./training-artifacts
          EXP_DIR="${{ steps.collect.outputs.experiment_dir }}"
          
          # Only copy lightweight files (model.pt stays on Occidata)
          cp "$EXP_DIR/metadata.json" ./training-artifacts/
          cp "$EXP_DIR/metrics.csv" ./training-artifacts/
          cp "$EXP_DIR/training_plot.png" ./training-artifacts/ || echo "plot not found"
          
          echo "ğŸ“¦ Prepared lightweight artifacts (model.pt excluded):"
          ls -lh ./training-artifacts/

      - name: ğŸ“¤ Upload training artifacts
        if: steps.collect.outputs.success == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: training-results-gpu
          path: ./training-artifacts/
          retention-days: 30

      - name: ğŸš€ Deploy model to registry
        id: deploy
        if: steps.collect.outputs.success == 'true'
        run: |
          cd blokus-engine
          source job_results.txt
          
          # Quality gate: only deploy if win_rate >= 0.5
          WIN_RATE_THRESHOLD=0.5
          if (( $(echo "$BEST_WIN_RATE >= $WIN_RATE_THRESHOLD" | bc -l) )); then
            echo "âœ… Model meets quality threshold (win_rate: $BEST_WIN_RATE >= $WIN_RATE_THRESHOLD)"
            
            # Create deployed directory if it doesn't exist
            mkdir -p models/deployed
            
            # Copy model to deployed directory
            EXP_NAME="${{ needs.setup.outputs.experiment_name }}"
            DEPLOYED_PATH="models/deployed/${EXP_NAME}.pt"
            cp "$EXPERIMENT_DIR/model.pt" "$DEPLOYED_PATH"
            echo "ğŸ“¦ Copied model to $DEPLOYED_PATH"
            
            # Update registry
            python ../scripts/update_registry.py \
              --id "$EXP_NAME" \
              --name "Expert Duo ($EXP_NAME)" \
              --description "IA entraÃ®nÃ©e sur GPU (Occidata). Win rate: ${BEST_WIN_RATE}. Niveau expert." \
              --model-path "deployed/${EXP_NAME}.pt" \
              --board-size ${{ needs.setup.outputs.board_size }} \
              --enabled
            
            echo "deployed=true" >> "$GITHUB_OUTPUT"
            echo "deployed_path=$DEPLOYED_PATH" >> "$GITHUB_OUTPUT"
          else
            echo "âš ï¸  Model does not meet quality threshold (win_rate: $BEST_WIN_RATE < $WIN_RATE_THRESHOLD)"
            echo "Model will not be deployed to registry"
            echo "deployed=false" >> "$GITHUB_OUTPUT"
          fi


  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Commit results
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  commit-results:
    name: ğŸ’¾ Commit training results
    runs-on: ubuntu-latest
    needs: [setup, train]
    if: needs.train.outputs.success == 'true'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: ğŸ“¥ Download training artifacts
        uses: actions/download-artifact@v4
        with:
          name: training-results-gpu
          path: ./training-artifacts

      - name: ğŸ“‚ Restore artifacts to correct location
        id: restore
        run: |
          # Create experiment directory with timestamp
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          EXP_NAME="${{ needs.setup.outputs.experiment_name }}_${TIMESTAMP}"
          TARGET_DIR="blokus-engine/models/experiments/${EXP_NAME}"
          mkdir -p "$TARGET_DIR"
          cp ./training-artifacts/* "$TARGET_DIR/"
          echo "experiment_dir=$TARGET_DIR" >> "$GITHUB_OUTPUT"
          echo "ğŸ“‚ Restored artifacts to $TARGET_DIR"
          ls -lh "$TARGET_DIR/"

      - name: ğŸ’¾ Commit and push results (lightweight files only)
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          EXP_DIR="${{ steps.restore.outputs.experiment_dir }}"
          EXP_NAME=$(basename "$EXP_DIR")
          
          # Remove model.pt if it was accidentally copied (it should stay on Occidata)
          rm -f "$EXP_DIR/model.pt" "$EXP_DIR/checkpoint_best.pt" "$EXP_DIR/checkpoint_latest.pt"
          
          # Add only lightweight experiment results
          git add "$EXP_DIR/metadata.json" "$EXP_DIR/metrics.csv" "$EXP_DIR/training_plot.png" 2>/dev/null || true
          
          git commit -m "ğŸ¤– GPU Training results: ${{ needs.setup.outputs.experiment_name }}
          
          Best win rate: ${{ needs.train.outputs.best_win_rate }}
          Trained on: Occidata GPU (GTX 1080 Ti)
          
          ğŸ“¦ Model stored on Occidata (not in Git due to size).
          To download: scp occidata:/projects/ctb/blokus-runner/trained_models/${EXP_NAME}/model.pt ./" || echo "No changes to commit"
          
          git push || echo "Nothing to push"
          
          echo ""
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "ğŸ“¦ MODEL DOWNLOAD INSTRUCTIONS"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""
          echo "The model.pt file is stored on Occidata persistent storage."
          echo "To download it, run:"
          echo ""
          echo "  scp occidata:/projects/ctb/blokus-runner/trained_models/${EXP_NAME}/model.pt ./"
          echo ""
          echo "Or to copy the entire experiment folder:"
          echo ""
          echo "  scp -r occidata:/projects/ctb/blokus-runner/trained_models/${EXP_NAME}/ ./"
          echo ""
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

